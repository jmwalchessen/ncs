{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/ \n",
    "\n",
    "import enum\n",
    "import math \n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.distributions import Normal \n",
    "import unet\n",
    "import sys\n",
    "sys.path.insert(1, '/home/juliatest/Dropbox/diffusion/twisted_diffusion/twisted_diffusion_sampler-main/smc_utils')\n",
    "import dist_util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#betas are from .0001 to .02 by increments of x for length 1000\n",
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    #same schedule as default in sde_lib\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return np.linspace(\n",
    "            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    "        )\n",
    "    elif schedule_name == \"cosine\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "    \n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "class LossType(enum.Enum):\n",
    "    MSE = enum.auto()  # use raw MSE loss (and KL when learning variances)\n",
    "    RESCALED_MSE = (\n",
    "        enum.auto()\n",
    "    )  # use raw MSE loss (with RESCALED_KL when learning variances)\n",
    "    KL = enum.auto()  # use the variational lower-bound\n",
    "    RESCALED_KL = enum.auto()  # like KL, but rescale to estimate the full VLB\n",
    "\n",
    "    def is_vb(self):\n",
    "        return self == LossType.KL or self == LossType.RESCALED_KL\n",
    "\n",
    "\n",
    "class ModelMeanType(enum.Enum):\n",
    "    \"\"\"\n",
    "    Which type of output the model predicts.\n",
    "    \"\"\"\n",
    "\n",
    "    PREVIOUS_X = enum.auto()  # the model predicts x_{t-1}\n",
    "    START_X = enum.auto()  # the model predicts x_0\n",
    "    EPSILON = enum.auto()  # the model predicts epsilon\n",
    "\n",
    "class ModelVarType(enum.Enum):\n",
    "    \"\"\"\n",
    "    What is used as the model's output variance.\n",
    "\n",
    "    The LEARNED_RANGE option has been added to allow the model to predict\n",
    "    values between FIXED_SMALL and FIXED_LARGE, making its job easier.\n",
    "    \"\"\"\n",
    "\n",
    "    LEARNED = enum.auto()\n",
    "    FIXED_SMALL = enum.auto()\n",
    "    FIXED_LARGE = enum.auto()\n",
    "    LEARNED_RANGE = enum.auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Utilities for training and sampling diffusion models.\n",
    "\n",
    "    Ported directly from here, and then adapted over time to further experimentation.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n",
    "                  starting at T and going to 1.\n",
    "    #this is probably from get_named_beta_schedule\n",
    "    :param model_mean_type: a ModelMeanType determining what the model outputs.\n",
    "    :param model_var_type: a ModelVarType determining how variance is output.\n",
    "    :param loss_type: a LossType determining the loss function to use.\n",
    "    :param rescale_timesteps: if True, pass floating point timesteps into the\n",
    "                              model so that they are always scaled like in the\n",
    "                              original paper (0 to 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        betas,\n",
    "        model_mean_type,\n",
    "        model_var_type,\n",
    "        loss_type,\n",
    "        rescale_timesteps=False,\n",
    "        conf=None\n",
    "    ):\n",
    "        self.model_mean_type = model_mean_type\n",
    "        self.model_var_type = model_var_type\n",
    "        self.loss_type = loss_type\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "\n",
    "        self.conf = conf\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas = alphas \n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_alphas_cumprod_prev = np.sqrt(self.alphas_cumprod_prev)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(\n",
    "            1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        #equation for ddpm x_t+1 | x_t,x_0 variance (see p. 3 of ddpm Ho paper)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) /\n",
    "            (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        #equation for ddpm x_t+1 | x_t,x_0 mean (part 1) (see p. 3 of ddpm Ho paper)\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) /\n",
    "            (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        #equation for ddpm x_t+1 | x_t,x_0 mean (part 2) (see p. 3 of ddpm Ho paper)\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    #equation from p 2 ddp Ho paper q(x_t | x_0)=N(sqrt alphabart*x_0, (1-alphabart)*I)\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = self.sqrt_alphas_cumprod[t] * x_start\n",
    "        variance = 1.0 - self.alphas_cumprod[t]\n",
    "        log_variance = self.log_one_minus_alphas_cumprod[t]\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    #sample from q(x_t | x_0)=N(sqrt alphabart*x_0, (1-alphabart)*I) by number of particles also give log prob\n",
    "    #of these samples\n",
    "    def q_sample(self, x_start, t, noise=None, num_particles=16, return_logprob=False):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        assert noise is None \n",
    "        mean = self.sqrt_alphas_cumprod[t] * x_start \n",
    "        #std deviation is scale\n",
    "        scale = self.sqrt_one_minus_alphas_cumprod[t] * x_start \n",
    "        # return samples, logprob (if required)\n",
    "        return _gaussian_sample(mean, scale=scale,\n",
    "                                sample_shape=th.Size([num_particles]), return_logprob=return_logprob)\n",
    "    #q(x_t+1 | x_t)=N(sqrt(alpha_tp1)*x_t, (1-alpha_tp1)) see p 2 of ddpm Ho\n",
    "    def q_sample_tp1_given_t(self, x_t, t):\n",
    "        \"\"\"\n",
    "        sample xtp1 given xt for t in [0, T-1] \n",
    "        \"\"\"\n",
    "        alphas_tp1 = self.alphas[t]  # due to python indexing \n",
    "        mean = np.sqrt(alphas_tp1) * x_t\n",
    "        scale = np.sqrt(1-alphas_tp1)\n",
    "        #scale is std dev (doesn't include x_t)\n",
    "        return mean + scale * th.randn_like(x_t)\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior:\n",
    "\n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        #see p 3 of DDPM Ho paper \n",
    "        posterior_mean = self.posterior_mean_coef1[t] * x_start \\\n",
    "            + self.posterior_mean_coef2[t] * x_t\n",
    "        posterior_variance = self.posterior_variance[t]\n",
    "        posterior_log_variance_clipped = self.posterior_log_variance_clipped[t]\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def q_posterior_sample(self, x_start, x_t, t, return_logprob=False, t_init=False):\n",
    "        posterior_mean, posterior_variance, posterior_log_variance_clipped = self.q_posterior_mean_variance(x_start, x_t, t)\n",
    "        if t_init:\n",
    "            #if first timestep return mean of posteror q(x_{t-1} | x_t,t)\n",
    "            out = (posterior_mean, th.zeros_like(posterior_mean))\n",
    "        else:\n",
    "            #if not, return a sample from q(x_{t-1} | x_t,t) = N(mu,sigma)\n",
    "            out =  _gaussian_sample(mean=posterior_mean, variance=posterior_variance,\n",
    "                                    return_logprob=return_logprob)\n",
    "        return out \n",
    "    \n",
    "    #sample from ddpm model i.e reverse process p(x_{t-1} | x_t) (i.e. return mean, variance, and predicted x0)\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        # x.shape: (P, C, H, W)\n",
    "        # t: a scalar \n",
    "\n",
    "        num_particles, C, H, W = x.shape\n",
    "\n",
    "        t_tensor = th.tensor([t] * num_particles, device=x.device)\n",
    "        y = model_kwargs.get(\"y\", None)\n",
    "        if y is not None:\n",
    "            y = y.expand(num_particles)\n",
    "        #get out predicted noise for given timestep conditioned on y if y is available\n",
    "        #in particular, first output is mean and second output is variance for the reverse process\n",
    "        #at that timestep\n",
    "        model_output = model(x, self._scale_timesteps(t_tensor), y=y) # **model_kwargs\n",
    "        \n",
    "\n",
    "        # default: LEARNED_RANGE\n",
    "        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n",
    "            assert model_output.shape == (num_particles, C * 2, H, W)\n",
    "            model_output = model_output.view(num_particles, C*2, H, W)\n",
    "            #split into mean and variance for reverse process at that timestep\n",
    "            model_output, model_var_values = th.split(model_output, C, dim=1)\n",
    " \n",
    "            if self.model_var_type == ModelVarType.LEARNED:\n",
    "                model_log_variance = model_var_values\n",
    "                model_variance = th.exp(model_log_variance)\n",
    "            else:\n",
    "                #This is what inpainting does because vartype = LEARNED RANGE\n",
    "                min_log = self.posterior_log_variance_clipped[t]\n",
    "                max_log = np.log(self.betas[t])\n",
    "                frac = (model_var_values + 1) / 2\n",
    "                model_log_variance = frac * max_log + (1 - frac) * min_log\n",
    "                model_variance = th.exp(model_log_variance)\n",
    "\n",
    "                # hack (need to set gaussian diffusion t_truncate = 0)\n",
    "                if self.t_truncate > 1 and t == self.t_truncate:\n",
    "                    _beta_t = 1 - self.alphas[t]\n",
    "                    _max_log = np.log(_beta_t)\n",
    "                    self._model_variance_at_t_truncate = th.exp(frac*_max_log)\n",
    "\n",
    "        else:\n",
    "            model_variance, model_log_variance = {\n",
    "                # for fixedlarge, we set the initial (log-)variance like so\n",
    "                # to get a better decoder log likelihood.\n",
    "                ModelVarType.FIXED_LARGE: (\n",
    "                    np.append(self.posterior_variance[1], self.betas[1:]),\n",
    "                    np.log(np.append(self.posterior_variance[1], self.betas[1:])),\n",
    "                ),\n",
    "                ModelVarType.FIXED_SMALL: (\n",
    "                    self.posterior_variance,\n",
    "                    self.posterior_log_variance_clipped,\n",
    "                ),\n",
    "            }[self.model_var_type]\n",
    "            model_variance =model_variance[t]\n",
    "            model_log_variance = model_log_variance[t]\n",
    "\n",
    "        #denoise input for processing step\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                x = denoised_fn(x)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output)\n",
    "            )\n",
    "            model_mean = model_output\n",
    "            #for inpainting, modelmeantype = EPSILON\n",
    "        elif self.model_mean_type in [ModelMeanType.START_X, ModelMeanType.EPSILON]:\n",
    "            if self.model_mean_type == ModelMeanType.START_X:\n",
    "                pred_xstart = process_xstart(model_output)\n",
    "            else:\n",
    "                #get denoised version of predicted x0\n",
    "                pred_xstart = process_xstart(\n",
    "                    self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "                )\n",
    "            #q(x_{t-1} | x_{t}, pred(x_0))\n",
    "            model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_mean_type)\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            \n",
    "                self.sqrt_recip_alphas_cumprod[t] * x_t\n",
    "            - self.sqrt_recipm1_alphas_cumprod[t] * eps\n",
    "        )\n",
    "    \n",
    "    #Not sure what this does yet and if it's relevant\n",
    "    def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n",
    "        \"\"\"\n",
    "        Compute the mean for the previous step, given a function cond_fn that\n",
    "        computes the gradient of a conditional log probability with respect to\n",
    "        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n",
    "        condition on y.\n",
    "\n",
    "        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n",
    "        \"\"\"\n",
    "\n",
    "        gradient = cond_fn(x, self._scale_timesteps(t), **model_kwargs)\n",
    "\n",
    "\n",
    "        new_mean = (\n",
    "            p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] *\n",
    "            gradient.float()\n",
    "        )\n",
    "        return new_mean\n",
    "    \n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t    \n",
    "        \n",
    "    def p_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param cond_fn: if not None, this is a gradient function that acts\n",
    "                        similarly to the model.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        noise = th.randn_like(x)\n",
    "\n",
    "        #returns mean, variance, log_variance, predicted x0 for x_{t-1}\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x[0].shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "\n",
    "        std = th.exp(0.5 * out[\"log_variance\"])\n",
    "        #get sample from N(pred(u_{t-1}), pred(sigma_{t-1}))\n",
    "        sample = out[\"mean\"] + nonzero_mask * \\\n",
    "            std * noise\n",
    "\n",
    "        return {\"sample\": sample, \n",
    "                \"mean\": out['mean'], \n",
    "                \"std\": std, \n",
    "                \"pred_xstart\": out['pred_xstart']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = get_named_beta_schedule(\"linear\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD = (GaussianDiffusion(betas = betas, model_mean_type = ModelMeanType.EPSILON,\n",
    "                  model_var_type = ModelVarType.LEARNED_RANGE, loss_type = LossType.MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = unet.UNetModel(image_size = 28,\n",
    "        in_channels = 1,\n",
    "        model_channels = 64,\n",
    "        out_channels = 2,\n",
    "        num_res_blocks = 3,\n",
    "        attention_resolutions = [1,2,4],\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 2, 2),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=4,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=True,\n",
    "        use_new_attention_order=False,\n",
    "        diffusion_steps=1000, \n",
    "        use_value_logger=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_folder = \"/home/juliatest/Dropbox/diffusion/twisted_diffusion/twisted_diffusion_sampler-main/image_exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.load_state_dict(\n",
    "        dist_util.load_state_dict((home_folder + \"/models/model060000.pt\")))\n",
    "unet_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (th.ones((1, 1, 28,28))).to(\"cuda:0\")\n",
    "t = (th.tensor([10])).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "GD.t_truncate = 0\n",
    "a = GD.p_mean_variance(unet_model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_tds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
